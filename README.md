# Numerical Methods for Functional Genomics

A comprehensive learning repository covering mathematical foundations for computational genomics, with applications in transcriptomics, DNA methylation, and cancer research.

## ğŸ¯ Learning Objectives

- Master calculus and optimization for machine learning
- Understand numerical linear algebra foundations
- Connect mathematical concepts to genomics tools (DESeq2, scVI, etc.)
- Build intuition through PyTorch implementations

## ğŸ“š Resources Used

| Book | Strength | Primary Use |
|------|----------|-------------|
| **Numerical Algorithms** (Solomon) | Rigorous proofs | Core algorithms |
| **Practical Linear Algebra** (Cohen) | Geometric intuition | Visualization |
| **Why Machines Learn** (Ananthaswamy) | Historical context | Understanding "why" |
| **Introduction to Statistical Learning** | Statistical perspective | Inference |
| **Machine Learning Algorithms in Depth** (Smolyakov) | Implementations | Code patterns |
| **Deep Learning with PyTorch** | Neural networks | DL applications |

## ğŸ“ Repository Structure

```
numerical-genomics-foundations/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ phase1-foundations/
â”‚   â”œâ”€â”€ module1.1-calculus-optimization/
â”‚   â”‚   â”œâ”€â”€ 01_gradients_derivatives.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_hessian_newton.ipynb
â”‚   â”‚   â”œâ”€â”€ 03_jacobian_backprop.ipynb
â”‚   â”‚   â”œâ”€â”€ 04_autodiff.ipynb
â”‚   â”‚   â””â”€â”€ 05_genomics_applications.ipynb
â”‚   â””â”€â”€ module1.2-linear-systems/
â”‚       â””â”€â”€ (coming soon)
â”œâ”€â”€ phase2-decompositions/
â”‚   â””â”€â”€ (SVD, PCA, eigendecomposition)
â”œâ”€â”€ phase3-optimization/
â”‚   â””â”€â”€ (advanced optimization methods)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ (sample datasets)
â””â”€â”€ utils/
    â””â”€â”€ (helper functions)
```

## ğŸš€ Getting Started

### Prerequisites

```bash
# Create conda environment
conda create -n genomics-math python=3.10
conda activate genomics-math

# Install dependencies
pip install -r requirements.txt
```

### Running Notebooks

```bash
# Start Jupyter
jupyter notebook

# Or use JupyterLab
jupyter lab
```

## ğŸ“– Phase 1: Foundations

### Module 1.1: Calculus & Optimization âœ…

| Notebook | Topics | Key Concepts |
|----------|--------|--------------|
| `01_gradients_derivatives` | Partial derivatives, gradient vector | âˆ‡f, directional derivative |
| `02_hessian_newton` | Second derivatives, optimization | Hessian, positive definiteness, Newton's method |
| `03_jacobian_backprop` | Vector-valued functions | Jacobian, chain rule, backpropagation |
| `04_autodiff` | Automatic differentiation | Computation graphs, forward/backward mode |
| `05_genomics_applications` | DESeq2, scVI internals | IRLS, condition numbers |

### Module 1.2: Linear Systems & Least Squares â³

Coming soon...

## ğŸ§¬ Genomics Connections

Throughout these notebooks, we connect math to real genomics:

- **Gradient descent** â†’ Neural network training in scVI
- **Hessian/Newton** â†’ DESeq2's IRLS algorithm
- **Condition numbers** â†’ Why some analyses fail
- **Jacobian** â†’ Backpropagation in gene expression autoencoders

## ğŸ“ License

Educational use. See individual resources for their licenses.

## ğŸ¤ Acknowledgments

Developed through interactive learning sessions, integrating multiple textbook perspectives with genomics applications.
