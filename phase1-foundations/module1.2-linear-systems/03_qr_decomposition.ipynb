{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: QR Decomposition\n",
    "\n",
    "**Module 1.2: Linear Systems & Least Squares**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand what QR decomposition is\n",
    "2. See why QR is more stable than normal equations\n",
    "3. Solve least squares using QR\n",
    "4. Use PyTorch's built-in QR functions\n",
    "\n",
    "## Resources\n",
    "- Solomon, *Numerical Algorithms*, Chapter 5\n",
    "- Cohen, *Practical Linear Algebra*, Chapter 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is QR Decomposition?\n",
    "\n",
    "Any matrix $X \\in \\mathbb{R}^{m \\times n}$ (with $m \\geq n$) can be factored as:\n",
    "\n",
    "$$X = QR$$\n",
    "\n",
    "Where:\n",
    "- $Q \\in \\mathbb{R}^{m \\times n}$ has **orthonormal columns**: $Q^\\top Q = I$\n",
    "- $R \\in \\mathbb{R}^{n \\times n}$ is **upper triangular**\n",
    "\n",
    "### Naming Convention\n",
    "- **Q**: \"Orthogonal\" (columns are orthonormal)\n",
    "- **R**: \"Right triangular\" (upper triangular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic QR decomposition\n",
    "X = torch.tensor([[1., 2.],\n",
    "                  [3., 4.],\n",
    "                  [5., 6.]], dtype=torch.float64)\n",
    "\n",
    "Q, R = torch.linalg.qr(X)\n",
    "\n",
    "print(\"Original X:\")\n",
    "print(X)\n",
    "print(f\"\\nShape: {X.shape}\")\n",
    "\n",
    "print(\"\\nQ (orthonormal columns):\")\n",
    "print(Q.round(decimals=4))\n",
    "print(f\"Shape: {Q.shape}\")\n",
    "\n",
    "print(\"\\nR (upper triangular):\")\n",
    "print(R.round(decimals=4))\n",
    "print(f\"Shape: {R.shape}\")\n",
    "\n",
    "# Verify Q'Q = I\n",
    "print(\"\\nQ'Q (should be identity):\")\n",
    "print((Q.T @ Q).round(decimals=10))\n",
    "\n",
    "# Verify QR = X\n",
    "print(\"\\nQR (should equal X):\")\n",
    "print((Q @ R).round(decimals=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Why QR for Least Squares?\n",
    "\n",
    "### Normal Equations (Unstable)\n",
    "\n",
    "$$\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$$\n",
    "\n",
    "Problem: Forms $X^\\top X$ which squares the condition number.\n",
    "\n",
    "### QR Approach (Stable)\n",
    "\n",
    "Starting from $X\\beta = y$, substitute $X = QR$:\n",
    "\n",
    "$$QR\\beta = y$$\n",
    "$$R\\beta = Q^\\top y$$\n",
    "\n",
    "Solve by back-substitution (R is triangular).\n",
    "\n",
    "**Key**: We never form $X^\\top X$, so we keep $\\kappa(X)$ instead of $\\kappa(X)^2$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_least_squares_normal(X, y):\n",
    "    \"\"\"Solve via normal equations (unstable).\"\"\"\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    return torch.linalg.solve(XtX, Xty)\n",
    "\n",
    "def solve_least_squares_qr(X, y):\n",
    "    \"\"\"Solve via QR decomposition (stable).\"\"\"\n",
    "    Q, R = torch.linalg.qr(X)\n",
    "    Qty = Q.T @ y\n",
    "    return torch.linalg.solve_triangular(R, Qty, upper=True)\n",
    "\n",
    "# Well-conditioned problem: both methods work\n",
    "X = torch.tensor([[1., 0.],\n",
    "                  [1., 1.],\n",
    "                  [1., 2.],\n",
    "                  [1., 3.]], dtype=torch.float64)\n",
    "y = torch.tensor([1., 2., 2., 4.], dtype=torch.float64)\n",
    "\n",
    "beta_normal = solve_least_squares_normal(X, y)\n",
    "beta_qr = solve_least_squares_qr(X, y)\n",
    "\n",
    "print(\"Well-conditioned problem:\")\n",
    "print(f\"  Normal equations: {beta_normal.numpy().round(6)}\")\n",
    "print(f\"  QR decomposition: {beta_qr.numpy().round(6)}\")\n",
    "print(f\"  Difference: {(beta_normal - beta_qr).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ill-conditioned problem: normal equations fail\n",
    "def create_ill_conditioned_problem(kappa):\n",
    "    \"\"\"Create a least squares problem with specified condition number.\"\"\"\n",
    "    m, n = 100, 5\n",
    "    \n",
    "    # Create X with known condition number\n",
    "    U, _ = torch.linalg.qr(torch.randn(m, n, dtype=torch.float64))\n",
    "    V, _ = torch.linalg.qr(torch.randn(n, n, dtype=torch.float64))\n",
    "    S = torch.logspace(0, -np.log10(kappa), n, dtype=torch.float64)\n",
    "    X = U @ torch.diag(S) @ V.T\n",
    "    \n",
    "    # True solution and response\n",
    "    beta_true = torch.randn(n, dtype=torch.float64)\n",
    "    y = X @ beta_true + 0.01 * torch.randn(m, dtype=torch.float64)\n",
    "    \n",
    "    return X, y, beta_true\n",
    "\n",
    "print(\"Comparison on ill-conditioned problems:\")\n",
    "print(f\"{'κ(X)':<12} {'κ(X\\'X)':<15} {'Error (Normal)':<18} {'Error (QR)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for kappa in [1e2, 1e4, 1e6, 1e8]:\n",
    "    X, y, beta_true = create_ill_conditioned_problem(kappa)\n",
    "    \n",
    "    beta_normal = solve_least_squares_normal(X, y)\n",
    "    beta_qr = solve_least_squares_qr(X, y)\n",
    "    \n",
    "    error_normal = (beta_normal - beta_true).norm() / beta_true.norm()\n",
    "    error_qr = (beta_qr - beta_true).norm() / beta_true.norm()\n",
    "    \n",
    "    print(f\"{kappa:<12.0e} {kappa**2:<15.0e} {error_normal.item():<18.2e} {error_qr.item():<15.2e}\")\n",
    "\n",
    "print(\"\\n→ QR remains stable even when normal equations fail!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Deriving the QR Solution\n",
    "\n",
    "Starting from normal equations:\n",
    "\n",
    "$$X^\\top X \\beta = X^\\top y$$\n",
    "\n",
    "Substitute $X = QR$:\n",
    "\n",
    "$$(QR)^\\top (QR) \\beta = (QR)^\\top y$$\n",
    "\n",
    "$$R^\\top Q^\\top Q R \\beta = R^\\top Q^\\top y$$\n",
    "\n",
    "Since $Q^\\top Q = I$:\n",
    "\n",
    "$$R^\\top R \\beta = R^\\top Q^\\top y$$\n",
    "\n",
    "If $R$ is invertible, multiply both sides by $(R^\\top)^{-1}$:\n",
    "\n",
    "$$R \\beta = Q^\\top y$$\n",
    "\n",
    "This is a triangular system - solve by back-substitution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step QR solution\n",
    "X = torch.tensor([[1., 0.],\n",
    "                  [1., 1.],\n",
    "                  [1., 2.],\n",
    "                  [1., 3.]], dtype=torch.float64)\n",
    "y = torch.tensor([1., 2., 2., 4.], dtype=torch.float64)\n",
    "\n",
    "print(\"Step-by-step QR solution:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: QR decomposition\n",
    "Q, R = torch.linalg.qr(X)\n",
    "print(\"\\nStep 1: X = QR\")\n",
    "print(f\"Q =\\n{Q.numpy().round(4)}\")\n",
    "print(f\"\\nR =\\n{R.numpy().round(4)}\")\n",
    "\n",
    "# Step 2: Compute Q'y\n",
    "Qty = Q.T @ y\n",
    "print(f\"\\nStep 2: Q'y = {Qty.numpy().round(4)}\")\n",
    "\n",
    "# Step 3: Solve R*beta = Q'y by back-substitution\n",
    "print(f\"\\nStep 3: Solve R·β = Q'y\")\n",
    "print(f\"  {R[0,0]:.4f}·β₀ + {R[0,1]:.4f}·β₁ = {Qty[0]:.4f}\")\n",
    "print(f\"  {R[1,0]:.4f}·β₀ + {R[1,1]:.4f}·β₁ = {Qty[1]:.4f}\")\n",
    "\n",
    "# Back-substitution\n",
    "beta = torch.zeros(2, dtype=torch.float64)\n",
    "beta[1] = Qty[1] / R[1, 1]  # From row 2\n",
    "beta[0] = (Qty[0] - R[0, 1] * beta[1]) / R[0, 0]  # From row 1\n",
    "\n",
    "print(f\"\\nSolution: β = {beta.numpy().round(4)}\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nVerification: X·β = {(X @ beta).numpy().round(4)}\")\n",
    "print(f\"Actual y = {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PyTorch's lstsq Function\n",
    "\n",
    "In practice, use `torch.linalg.lstsq()` which uses QR internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's built-in least squares\n",
    "X = torch.tensor([[1., 0.],\n",
    "                  [1., 1.],\n",
    "                  [1., 2.],\n",
    "                  [1., 3.]], dtype=torch.float64)\n",
    "y = torch.tensor([1., 2., 2., 4.], dtype=torch.float64)\n",
    "\n",
    "# lstsq returns: solution, residuals, rank, singular values\n",
    "result = torch.linalg.lstsq(X, y.unsqueeze(1))\n",
    "\n",
    "print(\"torch.linalg.lstsq output:\")\n",
    "print(f\"  Solution: {result.solution.squeeze().numpy().round(4)}\")\n",
    "print(f\"  Rank: {result.rank}\")\n",
    "\n",
    "# Compare with our QR solution\n",
    "beta_qr = solve_least_squares_qr(X, y)\n",
    "print(f\"  Our QR solution: {beta_qr.numpy().round(4)}\")\n",
    "print(\"  ✓ Match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Computational Cost Comparison\n",
    "\n",
    "| Method | Condition Number | Operations | Memory |\n",
    "|--------|-----------------|------------|--------|\n",
    "| Normal equations | $\\kappa(X)^2$ | $O(mn^2 + n^3)$ | $O(n^2)$ for $X^\\top X$ |\n",
    "| QR decomposition | $\\kappa(X)$ | $O(mn^2)$ | $O(mn)$ for Q |\n",
    "\n",
    "QR is slightly more expensive but much more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(m, n, n_trials=10):\n",
    "    \"\"\"Benchmark normal equations vs QR.\"\"\"\n",
    "    X = torch.randn(m, n, dtype=torch.float64)\n",
    "    y = torch.randn(m, dtype=torch.float64)\n",
    "    \n",
    "    # Normal equations\n",
    "    start = time.time()\n",
    "    for _ in range(n_trials):\n",
    "        _ = solve_least_squares_normal(X, y)\n",
    "    time_normal = (time.time() - start) / n_trials\n",
    "    \n",
    "    # QR\n",
    "    start = time.time()\n",
    "    for _ in range(n_trials):\n",
    "        _ = solve_least_squares_qr(X, y)\n",
    "    time_qr = (time.time() - start) / n_trials\n",
    "    \n",
    "    return time_normal, time_qr\n",
    "\n",
    "print(\"Timing comparison (seconds):\")\n",
    "print(f\"{'Size':<20} {'Normal Eq':<15} {'QR':<15} {'Ratio':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for m, n in [(100, 10), (1000, 50), (5000, 100)]:\n",
    "    t_normal, t_qr = benchmark(m, n)\n",
    "    print(f\"{f'{m}×{n}':<20} {t_normal:<15.6f} {t_qr:<15.6f} {t_qr/t_normal:<10.2f}\")\n",
    "\n",
    "print(\"\\n→ QR is slightly slower but MUCH more stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Genomics Application: Stable Regression\n",
    "\n",
    "When fitting gene expression models, use QR for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gene expression regression\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "n_genes = 5\n",
    "\n",
    "# Design matrix with some correlation (realistic)\n",
    "# Intercept + Treatment + Batch + Age + BMI\n",
    "intercept = torch.ones(n_samples)\n",
    "treatment = torch.cat([torch.zeros(50), torch.ones(50)])\n",
    "batch = torch.cat([torch.zeros(25), torch.ones(25), torch.zeros(25), torch.ones(25)])\n",
    "age = torch.randn(n_samples) * 10 + 50\n",
    "bmi = age * 0.3 + torch.randn(n_samples) * 5  # Correlated with age!\n",
    "\n",
    "X = torch.stack([intercept, treatment, batch, age, bmi], dim=1).double()\n",
    "\n",
    "# Check condition number\n",
    "U, S, Vh = torch.linalg.svd(X)\n",
    "kappa = S[0] / S[-1]\n",
    "print(f\"Design matrix condition number: {kappa:.1f}\")\n",
    "\n",
    "# Simulate gene expression for 5 genes\n",
    "true_betas = torch.randn(5, n_genes, dtype=torch.float64)\n",
    "Y = X @ true_betas + 0.5 * torch.randn(n_samples, n_genes, dtype=torch.float64)\n",
    "\n",
    "# Solve for each gene using QR\n",
    "print(f\"\\nFitting {n_genes} genes using QR decomposition...\")\n",
    "\n",
    "Q, R = torch.linalg.qr(X)\n",
    "beta_estimates = torch.linalg.solve_triangular(R, Q.T @ Y, upper=True)\n",
    "\n",
    "print(f\"\\nTrue vs Estimated coefficients for Gene 1:\")\n",
    "print(f\"  True:      {true_betas[:, 0].numpy().round(3)}\")\n",
    "print(f\"  Estimated: {beta_estimates[:, 0].numpy().round(3)}\")\n",
    "\n",
    "# Overall error\n",
    "error = (beta_estimates - true_betas).norm() / true_betas.norm()\n",
    "print(f\"\\nRelative error across all genes: {error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Manual QR\n",
    "Use Gram-Schmidt to compute QR of a 3×2 matrix. Compare with PyTorch.\n",
    "\n",
    "### Exercise 2: Stability Test\n",
    "Create increasingly ill-conditioned problems and compare normal equations vs QR error.\n",
    "\n",
    "### Exercise 3: DESeq2 Simulation\n",
    "Simulate the per-gene regression that DESeq2 performs using QR decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| QR decomposition | $X = QR$, Q orthonormal, R triangular |\n",
    "| Stability | Uses $\\kappa(X)$ not $\\kappa(X)^2$ |\n",
    "| Solution | $R\\beta = Q^\\top y$ (back-substitution) |\n",
    "| Practice | Use `torch.linalg.lstsq()` |\n",
    "\n",
    "## Next: 04_regularization.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
