{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Jacobian Matrix and Backpropagation\n",
    "\n",
    "**Module 1.1: Calculus & Optimization**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Compute the Jacobian matrix for vector-valued functions\n",
    "2. Understand the chain rule for Jacobians\n",
    "3. Connect Jacobians to neural network backpropagation\n",
    "4. Compute Jacobians for common layer types (linear, ReLU)\n",
    "\n",
    "## Resources\n",
    "- Solomon, *Numerical Algorithms*, §1.4.2\n",
    "- Ananthaswamy, *Why Machines Learn*, Chapter 5\n",
    "- ISLR, §10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Gradient vs Jacobian vs Hessian\n",
    "\n",
    "| | Function Type | Output Shape | Use Case |\n",
    "|---|--------------|--------------|----------|\n",
    "| **Gradient** | $f: \\mathbb{R}^n \\to \\mathbb{R}$ | Vector $(n,)$ | Loss functions |\n",
    "| **Jacobian** | $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ | Matrix $(m \\times n)$ | NN layers, transformations |\n",
    "| **Hessian** | $f: \\mathbb{R}^n \\to \\mathbb{R}$ | Matrix $(n \\times n)$ | Curvature, Newton's method |\n",
    "\n",
    "### The Jacobian Matrix\n",
    "\n",
    "For $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, the Jacobian is:\n",
    "\n",
    "$$Df = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "**Shape:** (number of outputs) × (number of inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple vector-valued function\n",
    "def f(x):\n",
    "    \"\"\"f: R² → R³\"\"\"\n",
    "    return torch.stack([\n",
    "        x[0]**2 + x[1],      # f1 = x² + y\n",
    "        x[0] * x[1],          # f2 = xy\n",
    "        torch.sin(x[0])       # f3 = sin(x)\n",
    "    ])\n",
    "\n",
    "# Compute Jacobian at a point\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "J = jacobian(f, x)\n",
    "\n",
    "print(\"Function: f(x,y) = [x² + y, xy, sin(x)]\")\n",
    "print(f\"\\nInput shape: {x.shape} (n=2)\")\n",
    "print(f\"Output shape: {f(x).shape} (m=3)\")\n",
    "print(f\"Jacobian shape: {J.shape} (m×n = 3×2)\")\n",
    "\n",
    "print(\"\\nJacobian at (1, 2):\")\n",
    "print(J)\n",
    "\n",
    "print(\"\\nVerification (by hand):\")\n",
    "print(\"∂f1/∂x = 2x = 2,  ∂f1/∂y = 1\")\n",
    "print(\"∂f2/∂x = y = 2,   ∂f2/∂y = x = 1\")\n",
    "print(f\"∂f3/∂x = cos(x) = {np.cos(1):.4f}, ∂f3/∂y = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Jacobian of a Linear Layer\n",
    "\n",
    "A neural network linear layer:\n",
    "\n",
    "$$f(\\vec{x}) = W\\vec{x} + \\vec{b}$$\n",
    "\n",
    "### Key Result\n",
    "\n",
    "**The Jacobian of a linear layer is just the weight matrix!**\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial \\vec{x}} = W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: Jacobian of linear layer = W\n",
    "W = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0],\n",
    "                  [5.0, 6.0]])\n",
    "b = torch.tensor([0.1, 0.2, 0.3])\n",
    "\n",
    "def linear_layer(x):\n",
    "    return W @ x + b\n",
    "\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "J = jacobian(linear_layer, x)\n",
    "\n",
    "print(\"Weight matrix W:\")\n",
    "print(W)\n",
    "print(\"\\nJacobian of linear layer:\")\n",
    "print(J)\n",
    "print(\"\\n✓ Jacobian = W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Chain Rule for Jacobians\n",
    "\n",
    "For composed functions $f = h \\circ g$:\n",
    "\n",
    "$$J_f = J_h \\cdot J_g$$\n",
    "\n",
    "**Matrix multiplication!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate chain rule\n",
    "W1 = torch.randn(3, 2)  # Layer 1: R² → R³\n",
    "W2 = torch.randn(4, 3)  # Layer 2: R³ → R⁴\n",
    "\n",
    "def g(x): return W1 @ x\n",
    "def h(z): return W2 @ z\n",
    "def f(x): return h(g(x))\n",
    "\n",
    "x = torch.randn(2)\n",
    "\n",
    "J_f_direct = jacobian(f, x)\n",
    "J_f_chain = jacobian(h, g(x)) @ jacobian(g, x)\n",
    "\n",
    "print(f\"Direct Jacobian:\\n{J_f_direct}\")\n",
    "print(f\"\\nChain rule J_h @ J_g:\\n{J_f_chain}\")\n",
    "print(f\"\\n✓ Match! Diff: {(J_f_direct - J_f_chain).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Jacobian of Element-wise Functions\n",
    "\n",
    "**Element-wise functions have diagonal Jacobians!**\n",
    "\n",
    "For ReLU:\n",
    "$$J_{\\text{ReLU}} = \\text{diag}(\\mathbb{1}_{z_1 > 0}, \\mathbb{1}_{z_2 > 0}, \\ldots)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian of ReLU\n",
    "z = torch.tensor([2.0, -1.0, 3.0, -0.5])\n",
    "J_relu = jacobian(torch.relu, z)\n",
    "\n",
    "print(f\"Input z: {z.tolist()}\")\n",
    "print(f\"ReLU(z): {torch.relu(z).tolist()}\")\n",
    "print(f\"\\nJacobian (diagonal matrix):\")\n",
    "print(J_relu)\n",
    "print(\"\\n→ Dead neurons (z ≤ 0) block gradient flow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Full Neural Network Layer\n",
    "\n",
    "For $f(\\vec{x}) = \\sigma(W\\vec{x} + \\vec{b})$:\n",
    "\n",
    "$$J_f = J_\\sigma \\cdot W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full layer with activation\n",
    "W = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "b = torch.tensor([0.1, -0.5, 0.3])\n",
    "\n",
    "def layer_with_relu(x):\n",
    "    return torch.relu(W @ x + b)\n",
    "\n",
    "x = torch.tensor([0.1, 0.1])\n",
    "z = W @ x + b  # Pre-activation\n",
    "\n",
    "J = jacobian(layer_with_relu, x)\n",
    "J_manual = torch.diag((z > 0).float()) @ W\n",
    "\n",
    "print(f\"Pre-activation z: {z.tolist()}\")\n",
    "print(f\"\\nJacobian:\\n{J}\")\n",
    "print(f\"\\n✓ Row 2 zeroed (z[1]={z[1]:.2f} < 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Backpropagation Through a Network\n",
    "\n",
    "Gradients flow backward through Jacobians:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\vec{x}} = \\frac{\\partial L}{\\partial \\vec{y}} \\cdot J_{\\text{layer}_n} \\cdot \\ldots \\cdot J_{\\text{layer}_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 3), nn.ReLU(),\n",
    "            nn.Linear(3, 2), nn.ReLU(),\n",
    "            nn.Linear(2, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.layers(x)\n",
    "\n",
    "net = SimpleNet()\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "y = net(x)\n",
    "loss = y**2\n",
    "loss.backward()\n",
    "\n",
    "print(f\"∂L/∂x = {x.grad.tolist()}\")\n",
    "\n",
    "# Verify with full Jacobian\n",
    "J_net = jacobian(lambda x: net(x), torch.tensor([1.0, 2.0]))\n",
    "grad_manual = 2 * net(torch.tensor([1.0, 2.0])) * J_net\n",
    "print(f\"Manual:  {grad_manual.squeeze().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Manual Jacobian\n",
    "Compute the Jacobian of $f(x,y) = (x^2 + y, xy, e^x)$ by hand at $(1, 2)$.\n",
    "\n",
    "### Exercise 2: Chain Rule\n",
    "For a 4-layer network with shapes 3→5→4→2→1, what is the shape of the full Jacobian $\\partial y / \\partial x$?\n",
    "\n",
    "### Exercise 3: Dead Neurons\n",
    "Create an input where ALL neurons in a ReLU layer are dead. What is the Jacobian?\n",
    "\n",
    "### Exercise 4: Sigmoid Saturation\n",
    "Compute the Jacobian of sigmoid at $z = 10$. Why is this problematic for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| **Jacobian shape** | (outputs × inputs) = $(m \\times n)$ |\n",
    "| **Linear layer** | Jacobian = $W$ |\n",
    "| **Chain rule** | $J_f = J_h \\cdot J_g$ (matrix multiply) |\n",
    "| **Element-wise** | Diagonal Jacobian |\n",
    "| **ReLU** | 0/1 diagonal (gates gradients) |\n",
    "| **Backprop** | Chain Jacobians backward |\n",
    "\n",
    "## Next: 04_autodiff.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
