{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Gradients and Derivatives\n",
    "\n",
    "**Module 1.1: Calculus & Optimization**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand partial derivatives and their geometric meaning\n",
    "2. Compute gradients by hand and with PyTorch\n",
    "3. Use directional derivatives to analyze functions\n",
    "4. Connect these concepts to genomics loss functions\n",
    "\n",
    "## Resources\n",
    "- Solomon, *Numerical Algorithms*, §1.4.1-1.4.2\n",
    "- Cohen, *Practical Linear Algebra*, Chapter 2\n",
    "- Ananthaswamy, *Why Machines Learn*, Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. From Single Variable to Multiple Variables\n",
    "\n",
    "### Single Variable Review\n",
    "\n",
    "For $f(x): \\mathbb{R} \\to \\mathbb{R}$, the derivative is:\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**Interpretation:** Rate of change of $f$ as $x$ changes.\n",
    "\n",
    "### Multiple Variables: Partial Derivatives\n",
    "\n",
    "For $f(x_1, x_2, ..., x_n): \\mathbb{R}^n \\to \\mathbb{R}$, we have **partial derivatives**:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_k} = \\lim_{h \\to 0} \\frac{f(x_1, ..., x_k + h, ..., x_n) - f(x_1, ..., x_k, ..., x_n)}{h}$$\n",
    "\n",
    "**Key idea:** Hold all other variables constant, differentiate with respect to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Computing Partial Derivatives\n",
    "\n",
    "Let $f(x, y) = x^2 + 3xy + 2y^2$\n",
    "\n",
    "**Partial with respect to $x$** (treat $y$ as constant):\n",
    "$$\\frac{\\partial f}{\\partial x} = 2x + 3y$$\n",
    "\n",
    "**Partial with respect to $y$** (treat $x$ as constant):\n",
    "$$\\frac{\\partial f}{\\partial y} = 3x + 4y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with PyTorch\n",
    "def f(x, y):\n",
    "    return x**2 + 3*x*y + 2*y**2\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Compute function value\n",
    "z = f(x, y)\n",
    "print(f\"f(2, 3) = {z.item()}\")\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(f\"∂f/∂x at (2,3) = {x.grad.item()}  (Expected: 2*2 + 3*3 = 13)\")\n",
    "print(f\"∂f/∂y at (2,3) = {y.grad.item()}  (Expected: 3*2 + 4*3 = 18)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Gradient Vector\n",
    "\n",
    "The **gradient** collects all partial derivatives into a vector:\n",
    "\n",
    "$$\\nabla f(\\vec{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Direction:** $\\nabla f$ points in the direction of steepest ascent\n",
    "2. **Magnitude:** $\\|\\nabla f\\|$ gives the rate of steepest ascent\n",
    "3. **Perpendicular to level sets:** $\\nabla f$ is perpendicular to contour lines $f(\\vec{x}) = c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient as direction of steepest ascent\n",
    "def f_2d(xy):\n",
    "    x, y = xy[0], xy[1]\n",
    "    return x**2 + 2*y**2  # Elliptical bowl\n",
    "\n",
    "# Create grid for contour plot\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "y_range = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 + 2*Y**2\n",
    "\n",
    "# Compute gradient at several points\n",
    "points = [(-2, 1), (1, 2), (2, -1), (-1, -1.5)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "contours = ax.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "# Plot gradients at each point\n",
    "for px, py in points:\n",
    "    # Gradient: (2x, 4y)\n",
    "    grad_x, grad_y = 2*px, 4*py\n",
    "    \n",
    "    # Normalize for visualization (but show actual direction)\n",
    "    scale = 0.3\n",
    "    ax.arrow(px, py, scale*grad_x, scale*grad_y, \n",
    "             head_width=0.15, head_length=0.1, fc='red', ec='red')\n",
    "    ax.plot(px, py, 'ko', markersize=8)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Gradient vectors (red) point perpendicular to contour lines\\n(direction of steepest ascent)')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Directional Derivative\n",
    "\n",
    "The **directional derivative** measures the rate of change of $f$ in any direction $\\vec{v}$:\n",
    "\n",
    "$$D_{\\vec{v}}f(\\vec{x}) = \\nabla f(\\vec{x}) \\cdot \\vec{v} = \\|\\nabla f\\| \\|\\vec{v}\\| \\cos\\theta$$\n",
    "\n",
    "where $\\theta$ is the angle between $\\nabla f$ and $\\vec{v}$.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "- **Maximum** when $\\vec{v}$ aligns with $\\nabla f$ ($\\theta = 0$)\n",
    "- **Zero** when $\\vec{v}$ is perpendicular to $\\nabla f$ ($\\theta = 90°$)\n",
    "- **Minimum** when $\\vec{v}$ is opposite to $\\nabla f$ ($\\theta = 180°$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute directional derivative\n",
    "def directional_derivative(f, x, v):\n",
    "    \"\"\"\n",
    "    Compute directional derivative of f at x in direction v.\n",
    "    \n",
    "    Args:\n",
    "        f: Function that takes a tensor and returns a scalar\n",
    "        x: Point (tensor with requires_grad=True)\n",
    "        v: Direction vector (tensor)\n",
    "    \n",
    "    Returns:\n",
    "        Directional derivative (scalar)\n",
    "    \"\"\"\n",
    "    x = x.clone().requires_grad_(True)\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    grad = x.grad\n",
    "    \n",
    "    # D_v f = grad · v\n",
    "    return torch.dot(grad, v).item()\n",
    "\n",
    "# Example: f(x,y) = x^2 + 2y^2\n",
    "def f_bowl(xy):\n",
    "    return xy[0]**2 + 2*xy[1]**2\n",
    "\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "\n",
    "# Gradient at (1,1) is (2, 4)\n",
    "# Test different directions\n",
    "directions = {\n",
    "    'Along gradient (2,4) normalized': torch.tensor([2.0, 4.0]) / torch.norm(torch.tensor([2.0, 4.0])),\n",
    "    'Opposite to gradient': -torch.tensor([2.0, 4.0]) / torch.norm(torch.tensor([2.0, 4.0])),\n",
    "    'Perpendicular (4,-2) normalized': torch.tensor([4.0, -2.0]) / torch.norm(torch.tensor([4.0, -2.0])),\n",
    "    'x-direction (1,0)': torch.tensor([1.0, 0.0]),\n",
    "}\n",
    "\n",
    "print(\"Directional derivatives at (1,1):\")\n",
    "print(\"Gradient = (2, 4), |∇f| = \", torch.norm(torch.tensor([2.0, 4.0])).item())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, v in directions.items():\n",
    "    dd = directional_derivative(f_bowl, x, v)\n",
    "    print(f\"{name}: {dd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Norms: Measuring Magnitude\n",
    "\n",
    "Different norms measure vector \"size\" differently:\n",
    "\n",
    "| Norm | Formula | Interpretation |\n",
    "|------|---------|----------------|\n",
    "| $L_1$ | $\\|\\vec{x}\\|_1 = \\sum_i \\|x_i\\|$ | Manhattan distance |\n",
    "| $L_2$ | $\\|\\vec{x}\\|_2 = \\sqrt{\\sum_i x_i^2}$ | Euclidean distance |\n",
    "| $L_\\infty$ | $\\|\\vec{x}\\|_\\infty = \\max_i \\|x_i\\|$ | Maximum element |\n",
    "\n",
    "### Why This Matters for Genomics\n",
    "\n",
    "- **L2 norm:** Used in ridge regression, PCA\n",
    "- **L1 norm:** Used in LASSO for sparse gene selection\n",
    "- **L∞ norm:** Used for numerical stability checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norms(x):\n",
    "    \"\"\"Compute L1, L2, and Linf norms of a vector.\"\"\"\n",
    "    return {\n",
    "        'L1': torch.norm(x, p=1).item(),\n",
    "        'L2': torch.norm(x, p=2).item(),\n",
    "        'Linf': torch.norm(x, p=float('inf')).item()\n",
    "    }\n",
    "\n",
    "# Example: Gene expression difference vector\n",
    "# Imagine comparing expression of 5 genes between two conditions\n",
    "gene_diff = torch.tensor([0.5, -2.0, 0.1, 3.0, -0.3])\n",
    "\n",
    "print(\"Gene expression differences:\", gene_diff.tolist())\n",
    "print(\"\\nNorms:\")\n",
    "for name, value in compute_norms(gene_diff).items():\n",
    "    print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  L1: Total absolute change across all genes\")\n",
    "print(\"  L2: Overall magnitude (Euclidean)\")\n",
    "print(\"  Linf: Largest single gene change (gene 4 = 3.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Gradient of Common Functions\n",
    "\n",
    "### Important Formulas (memorize these!)\n",
    "\n",
    "| Function | Gradient |\n",
    "|----------|----------|\n",
    "| $f(\\vec{x}) = \\vec{a}^\\top \\vec{x}$ | $\\nabla f = \\vec{a}$ |\n",
    "| $f(\\vec{x}) = \\vec{x}^\\top A \\vec{x}$ | $\\nabla f = (A + A^\\top)\\vec{x}$ |\n",
    "| $f(\\vec{x}) = \\vec{x}^\\top A \\vec{x}$ (symmetric $A$) | $\\nabla f = 2A\\vec{x}$ |\n",
    "| $f(\\vec{x}) = \\|\\vec{x}\\|_2^2$ | $\\nabla f = 2\\vec{x}$ |\n",
    "\n",
    "### MSE Loss Gradient (Critical for ML!)\n",
    "\n",
    "For predictions $\\hat{y} = X\\vec{w}$ and targets $\\vec{y}$:\n",
    "\n",
    "$$L(\\vec{w}) = \\frac{1}{n}\\|X\\vec{w} - \\vec{y}\\|_2^2$$\n",
    "\n",
    "$$\\nabla_\\vec{w} L = \\frac{2}{n}X^\\top(X\\vec{w} - \\vec{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify MSE gradient formula\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Simulate gene expression data\n",
    "n_samples = 100\n",
    "n_features = 5  # e.g., 5 covariates\n",
    "\n",
    "X = torch.randn(n_samples, n_features)  # Design matrix\n",
    "y = torch.randn(n_samples)               # Target (e.g., gene expression)\n",
    "w = torch.randn(n_features, requires_grad=True)  # Coefficients\n",
    "\n",
    "# MSE Loss\n",
    "def mse_loss(w, X, y):\n",
    "    residuals = X @ w - y\n",
    "    return torch.mean(residuals**2)\n",
    "\n",
    "# Compute gradient with autograd\n",
    "loss = mse_loss(w, X, y)\n",
    "loss.backward()\n",
    "autograd_gradient = w.grad.clone()\n",
    "\n",
    "# Compute gradient with formula\n",
    "with torch.no_grad():\n",
    "    residuals = X @ w - y\n",
    "    formula_gradient = (2/n_samples) * X.T @ residuals\n",
    "\n",
    "print(\"Gradient comparison (should match):\")\n",
    "print(f\"Autograd: {autograd_gradient.numpy().round(4)}\")\n",
    "print(f\"Formula:  {formula_gradient.numpy().round(4)}\")\n",
    "print(f\"\\nMax difference: {(autograd_gradient - formula_gradient).abs().max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Genomics Application: Gene Expression Loss Function\n",
    "\n",
    "In differential expression analysis, we often minimize:\n",
    "\n",
    "$$L(\\beta) = -\\log P(\\text{data} | \\beta)$$\n",
    "\n",
    "For a simplified Gaussian model:\n",
    "\n",
    "$$L(\\beta) = \\sum_{i=1}^{n} (y_i - X_i^\\top \\beta)^2$$\n",
    "\n",
    "Let's compute gradients for a toy gene expression example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated differential expression analysis\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Simulate: 50 samples, 2 conditions (control vs treatment)\n",
    "n_samples = 50\n",
    "\n",
    "# Design matrix: intercept + treatment indicator\n",
    "X = torch.zeros(n_samples, 2)\n",
    "X[:, 0] = 1  # Intercept\n",
    "X[:25, 1] = 0  # Control\n",
    "X[25:, 1] = 1  # Treatment\n",
    "\n",
    "# True coefficients: baseline=5, treatment effect=2\n",
    "beta_true = torch.tensor([5.0, 2.0])\n",
    "\n",
    "# Simulated gene expression with noise\n",
    "y = X @ beta_true + 0.5 * torch.randn(n_samples)\n",
    "\n",
    "# Starting guess\n",
    "beta = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "\n",
    "# Compute loss and gradient\n",
    "loss = torch.mean((X @ beta - y)**2)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Simulated Gene Expression Analysis\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"True coefficients: {beta_true.tolist()}\")\n",
    "print(f\"Initial guess: {beta.detach().tolist()}\")\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "print(f\"Gradient at initial guess: {beta.grad.tolist()}\")\n",
    "print(f\"\\nGradient interpretation:\")\n",
    "print(f\"  ∂L/∂β₀ = {beta.grad[0].item():.2f} → Intercept should {'increase' if beta.grad[0] < 0 else 'decrease'}\")\n",
    "print(f\"  ∂L/∂β₁ = {beta.grad[1].item():.2f} → Treatment effect should {'increase' if beta.grad[1] < 0 else 'decrease'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Manual Gradient Computation\n",
    "\n",
    "Compute the gradient of $f(x, y, z) = x^2y + yz^2 + xz$ by hand, then verify with PyTorch.\n",
    "\n",
    "### Exercise 2: Directional Derivative\n",
    "\n",
    "For $f(x, y) = x^2 - xy + y^2$ at point $(1, 2)$:\n",
    "1. Compute $\\nabla f$\n",
    "2. Find the directional derivative in direction $(3, 4)$ (normalize first!)\n",
    "3. What direction gives the maximum rate of increase?\n",
    "\n",
    "### Exercise 3: Norm Comparison\n",
    "\n",
    "Generate a random vector of length 100. Compare L1, L2, and L∞ norms. Which is largest? Smallest? Why?\n",
    "\n",
    "### Exercise 4: MSE Gradient\n",
    "\n",
    "Derive the gradient of the MSE loss $L(\\vec{w}) = \\|X\\vec{w} - \\vec{y}\\|_2^2$ step by step using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "# f(x, y, z) = x²y + yz² + xz\n",
    "# ∂f/∂x = ?\n",
    "# ∂f/∂y = ?\n",
    "# ∂f/∂z = ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Formula | Interpretation |\n",
    "|---------|-------------|----------------|\n",
    "| Partial derivative | $\\frac{\\partial f}{\\partial x_k}$ | Rate of change in one variable |\n",
    "| Gradient | $\\nabla f = (\\frac{\\partial f}{\\partial x_1}, ..., \\frac{\\partial f}{\\partial x_n})$ | Direction of steepest ascent |\n",
    "| Directional derivative | $D_{\\vec{v}}f = \\nabla f \\cdot \\vec{v}$ | Rate of change in direction $\\vec{v}$ |\n",
    "| MSE gradient | $\\frac{2}{n}X^\\top(X\\vec{w} - \\vec{y})$ | Used in linear regression |\n",
    "\n",
    "## Next Notebook\n",
    "\n",
    "**02_hessian_newton.ipynb:** Second derivatives, the Hessian matrix, and Newton's optimization method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
