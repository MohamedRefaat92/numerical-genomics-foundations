{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: Automatic Differentiation\n",
    "\n",
    "**Module 1.1: Calculus & Optimization**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand why numerical differentiation fails at scale\n",
    "2. Learn how automatic differentiation (autodiff) works\n",
    "3. Trace through computation graphs manually\n",
    "4. Understand forward vs backward mode autodiff\n",
    "5. See how PyTorch's autograd implements backpropagation\n",
    "\n",
    "## Resources\n",
    "- Solomon, *Numerical Algorithms*, §14.3.5\n",
    "- Ananthaswamy, *Why Machines Learn*, Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Three Ways to Compute Derivatives\n",
    "\n",
    "| Method | Accuracy | Speed | How |\n",
    "|--------|----------|-------|-----|\n",
    "| **Numerical** | Limited | $O(n)$ evals | Finite differences |\n",
    "| **Symbolic** | Exact | Expression explosion | Algebraic rules |\n",
    "| **Autodiff** | Machine precision | $O(1)$ passes | Chain rule on graph |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Why Numerical Differentiation Fails\n",
    "\n",
    "Numerical differentiation: $f'(x) \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}$\n",
    "\n",
    "### Two Problems:\n",
    "1. **Precision:** Too small $\\epsilon$ → floating point errors\n",
    "2. **Speed:** Need $O(n)$ function evaluations for $n$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Precision issues\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "x = 1.0\n",
    "true_derivative = 2.0  # f'(x) = 2x\n",
    "\n",
    "print(\"Numerical differentiation precision:\")\n",
    "print(f\"{'epsilon':<15} {'approx deriv':<15} {'error':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for exp in range(1, 17):\n",
    "    eps = 10**(-exp)\n",
    "    approx = (f(x + eps) - f(x)) / eps\n",
    "    error = abs(approx - true_derivative)\n",
    "    print(f\"1e-{exp:<13} {approx:<15.10f} {error:<15.2e}\")\n",
    "\n",
    "print(\"\\n⚠️ Error increases for very small epsilon (floating point issues)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Speed - O(n) evaluations\n",
    "def numerical_gradient(f, x, eps=1e-7):\n",
    "    \"\"\"Compute gradient numerically - requires n+1 function evals.\"\"\"\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    f_x = f(x)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        grad[i] = (f(x_plus) - f_x) / eps\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# For n=1,000,000 parameters, need 1,000,001 function evaluations!\n",
    "print(\"Function evaluations needed for gradient:\")\n",
    "for n in [10, 100, 1000, 1000000]:\n",
    "    print(f\"  n = {n:>10,} params → {n+1:>10,} evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Automatic Differentiation: The Key Idea\n",
    "\n",
    "Autodiff builds a **computation graph** during the forward pass, then uses the **chain rule** to compute gradients in the backward pass.\n",
    "\n",
    "### Example: $y = x_0^2 + x_0 \\cdot x_1 + x_1^3$\n",
    "\n",
    "```\n",
    "x[0]=2    x[1]=3\n",
    "   │         │\n",
    "   ▼         ▼\n",
    "  (²)       (³)\n",
    "   │         │\n",
    "   ▼         ▼\n",
    "   4        27\n",
    "   │         │\n",
    "   │    ┌────┴────┐\n",
    "   │    │         │\n",
    "   │   (×)────────┤\n",
    "   │    │         │\n",
    "   │    6         │\n",
    "   │    │         │\n",
    "   └────┼─────────┘\n",
    "        │\n",
    "       (+)\n",
    "        │\n",
    "        ▼\n",
    "       y=37\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build computation manually\n",
    "x0, x1 = 2.0, 3.0\n",
    "\n",
    "# Forward pass - compute and store intermediate values\n",
    "a = x0**2       # a = 4\n",
    "b = x1**3       # b = 27\n",
    "c = x0 * x1     # c = 6\n",
    "y = a + c + b   # y = 37\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"  a = x0² = {a}\")\n",
    "print(f\"  b = x1³ = {b}\")\n",
    "print(f\"  c = x0·x1 = {c}\")\n",
    "print(f\"  y = a + c + b = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Backward Pass: Chain Rule\n",
    "\n",
    "Start with $\\frac{\\partial y}{\\partial y} = 1$ and propagate backward.\n",
    "\n",
    "At each node, multiply **upstream gradient** by **local derivative**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass - manual\n",
    "x0, x1 = 2.0, 3.0\n",
    "\n",
    "# Forward pass (storing intermediates)\n",
    "a = x0**2\n",
    "b = x1**3\n",
    "c = x0 * x1\n",
    "y = a + c + b\n",
    "\n",
    "# Backward pass\n",
    "dy_dy = 1.0  # Start here\n",
    "\n",
    "# y = a + c + b\n",
    "dy_da = dy_dy * 1  # ∂(a+c+b)/∂a = 1\n",
    "dy_dc = dy_dy * 1  # ∂(a+c+b)/∂c = 1\n",
    "dy_db = dy_dy * 1  # ∂(a+c+b)/∂b = 1\n",
    "\n",
    "# a = x0², so ∂a/∂x0 = 2*x0\n",
    "da_dx0 = 2 * x0  # = 4\n",
    "\n",
    "# b = x1³, so ∂b/∂x1 = 3*x1²\n",
    "db_dx1 = 3 * x1**2  # = 27\n",
    "\n",
    "# c = x0*x1, so ∂c/∂x0 = x1, ∂c/∂x1 = x0\n",
    "dc_dx0 = x1  # = 3\n",
    "dc_dx1 = x0  # = 2\n",
    "\n",
    "# Combine paths to x0 (from a and c)\n",
    "dy_dx0 = dy_da * da_dx0 + dy_dc * dc_dx0\n",
    "# = 1 * 4 + 1 * 3 = 7\n",
    "\n",
    "# Combine paths to x1 (from b and c)\n",
    "dy_dx1 = dy_db * db_dx1 + dy_dc * dc_dx1\n",
    "# = 1 * 27 + 1 * 2 = 29\n",
    "\n",
    "print(\"Backward pass (manual):\")\n",
    "print(f\"  ∂y/∂x0 = {dy_dx0}\")\n",
    "print(f\"  ∂y/∂x1 = {dy_dx1}\")\n",
    "\n",
    "# Verify with PyTorch\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x[0]**2 + x[0]*x[1] + x[1]**3\n",
    "y.backward()\n",
    "\n",
    "print(f\"\\nPyTorch autograd:\")\n",
    "print(f\"  ∂y/∂x0 = {x.grad[0].item()}\")\n",
    "print(f\"  ∂y/∂x1 = {x.grad[1].item()}\")\n",
    "print(\"\\n✓ Match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Forward vs Backward Mode\n",
    "\n",
    "| Mode | Direction | Cost | Best For |\n",
    "|------|-----------|------|----------|\n",
    "| **Forward** | Input → Output | $O(n)$ passes | Few inputs, many outputs |\n",
    "| **Backward** | Output → Input | $O(m)$ passes | Many inputs, few outputs |\n",
    "\n",
    "**Neural networks:** Millions of inputs (parameters), ONE output (loss)\n",
    "\n",
    "→ **Backward mode wins!** This is why it's called **back**propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Forward mode would need n passes\n",
    "# Backward mode needs 1 pass\n",
    "\n",
    "n_params = [10, 100, 1000, 10000, 100000, 1000000]\n",
    "\n",
    "print(\"Passes needed for full gradient:\")\n",
    "print(f\"{'# params':<12} {'Forward mode':<15} {'Backward mode':<15}\")\n",
    "print(\"-\" * 42)\n",
    "for n in n_params:\n",
    "    print(f\"{n:<12,} {n:<15,} {1:<15}\")\n",
    "\n",
    "print(\"\\n→ Backward mode is O(1) regardless of parameter count!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Forward Mode: Dual Numbers\n",
    "\n",
    "Forward autodiff uses \"dual numbers\": $[u, u']$ where $u' = \\frac{du}{dt}$\n",
    "\n",
    "### Rules:\n",
    "- $[u, u'] + [v, v'] = [u+v, u'+v']$\n",
    "- $[u, u'] \\cdot [v, v'] = [uv, uv' + u'v]$\n",
    "- $\\exp([u, u']) = [e^u, u'e^u]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNumber:\n",
    "    \"\"\"Simple dual number for forward-mode autodiff.\"\"\"\n",
    "    def __init__(self, value, deriv=0.0):\n",
    "        self.value = value\n",
    "        self.deriv = deriv\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return DualNumber(self.value + other, self.deriv)\n",
    "        return DualNumber(self.value + other.value, self.deriv + other.deriv)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return DualNumber(self.value * other, self.deriv * other)\n",
    "        return DualNumber(self.value * other.value, \n",
    "                         self.value * other.deriv + self.deriv * other.value)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "    \n",
    "    def __pow__(self, n):\n",
    "        return DualNumber(self.value**n, n * self.value**(n-1) * self.deriv)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Dual({self.value}, {self.deriv})\"\n",
    "\n",
    "# Example: f(x) = x³ + 2x, find f'(3)\n",
    "x = DualNumber(3, 1)  # x = 3, dx/dx = 1\n",
    "y = x**3 + 2*x\n",
    "\n",
    "print(f\"f(x) = x³ + 2x at x = 3\")\n",
    "print(f\"Result: {y}\")\n",
    "print(f\"f(3) = {y.value}, f'(3) = {y.deriv}\")\n",
    "print(f\"\\nVerify: f'(x) = 3x² + 2, f'(3) = 3(9) + 2 = 29 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. PyTorch Autograd Internals\n",
    "\n",
    "When you set `requires_grad=True`, PyTorch:\n",
    "1. Builds a computation graph during forward pass\n",
    "2. Stores `grad_fn` on each tensor (the backward function)\n",
    "3. Calls `.backward()` to traverse graph in reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek inside PyTorch's computation graph\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Build graph\n",
    "a = x[0]**2\n",
    "b = x[1]**3\n",
    "c = x[0] * x[1]\n",
    "y = a + b + c\n",
    "\n",
    "print(\"Computation graph:\")\n",
    "print(f\"  x.grad_fn = {x.grad_fn}\")\n",
    "print(f\"  a.grad_fn = {a.grad_fn}\")\n",
    "print(f\"  b.grad_fn = {b.grad_fn}\")\n",
    "print(f\"  c.grad_fn = {c.grad_fn}\")\n",
    "print(f\"  y.grad_fn = {y.grad_fn}\")\n",
    "\n",
    "print(\"\\nBackward graph (y's inputs):\")\n",
    "print(f\"  {y.grad_fn.next_functions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook to see gradients flowing through\n",
    "def print_grad(name):\n",
    "    def hook(grad):\n",
    "        print(f\"  Gradient at {name}: {grad}\")\n",
    "    return hook\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "a = x[0]**2\n",
    "b = x[1]**3  \n",
    "c = x[0] * x[1]\n",
    "\n",
    "# Register hooks\n",
    "a.register_hook(print_grad('a'))\n",
    "b.register_hook(print_grad('b'))\n",
    "c.register_hook(print_grad('c'))\n",
    "\n",
    "y = a + b + c\n",
    "\n",
    "print(\"Backward pass gradients:\")\n",
    "y.backward()\n",
    "print(f\"\\nFinal gradient: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Genomics Application: Gradient of VAE Loss\n",
    "\n",
    "In scVI, the loss combines reconstruction + KL divergence:\n",
    "\n",
    "$$L = -\\log p(x|z) + \\text{KL}(q(z|x) \\| p(z))$$\n",
    "\n",
    "Autodiff handles this complex loss automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified VAE loss\n",
    "def vae_loss(x, x_recon, mu, logvar):\n",
    "    \"\"\"VAE loss = Reconstruction + KL divergence.\"\"\"\n",
    "    # Reconstruction loss (MSE for simplicity)\n",
    "    recon_loss = torch.mean((x - x_recon)**2)\n",
    "    \n",
    "    # KL divergence: -0.5 * sum(1 + log(σ²) - μ² - σ²)\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu**2 - torch.exp(logvar))\n",
    "    \n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# Simulated data\n",
    "x = torch.randn(100)  # Gene expression\n",
    "x_recon = torch.randn(100, requires_grad=True)\n",
    "mu = torch.randn(10, requires_grad=True)\n",
    "logvar = torch.randn(10, requires_grad=True)\n",
    "\n",
    "loss = vae_loss(x, x_recon, mu, logvar)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"VAE Loss: {loss.item():.4f}\")\n",
    "print(f\"\\nGradients computed automatically:\")\n",
    "print(f\"  ∂L/∂x_recon shape: {x_recon.grad.shape}\")\n",
    "print(f\"  ∂L/∂mu shape: {mu.grad.shape}\")\n",
    "print(f\"  ∂L/∂logvar shape: {logvar.grad.shape}\")\n",
    "print(\"\\n✓ Complex loss, simple gradient computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Manual Backprop\n",
    "For $y = (x_1 + x_2) \\cdot x_3$ at $(1, 2, 3)$, trace through the backward pass manually.\n",
    "\n",
    "### Exercise 2: Dual Numbers\n",
    "Extend the DualNumber class to support `sin` and `cos`.\n",
    "\n",
    "### Exercise 3: Graph Inspection\n",
    "Build a 3-layer network and print the `grad_fn` chain.\n",
    "\n",
    "### Exercise 4: Gradient Checkpointing\n",
    "Research: What is gradient checkpointing and why is it useful for large models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| **Numerical diff** | $O(n)$ evals, precision issues |\n",
    "| **Autodiff** | Exact, $O(1)$ backward passes |\n",
    "| **Computation graph** | Stores values + local derivatives |\n",
    "| **Backward pass** | upstream × local, sum at branches |\n",
    "| **Forward mode** | Dual numbers $[u, u']$ |\n",
    "| **Backward mode** | Better for many params → 1 loss |\n",
    "\n",
    "## Next: 05_genomics_applications.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
