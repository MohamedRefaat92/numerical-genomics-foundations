{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Hessian Matrix and Newton's Method\n",
    "\n",
    "**Module 1.1: Calculus & Optimization**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Compute the Hessian matrix of second partial derivatives\n",
    "2. Understand positive/negative definiteness and their geometric meaning\n",
    "3. Classify critical points (minimum, maximum, saddle)\n",
    "4. Implement Newton's method for optimization\n",
    "5. Compare Newton's method to gradient descent\n",
    "\n",
    "## Resources\n",
    "- Solomon, *Numerical Algorithms*, §9.4.2\n",
    "- Cohen, *Practical Linear Algebra*, Chapter 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd.functional import hessian\n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Hessian Matrix\n",
    "\n",
    "The **Hessian** is the matrix of all second partial derivatives:\n",
    "\n",
    "$$H_f(\\vec{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Shape:** $n \\times n$ for a function of $n$ variables\n",
    "2. **Symmetric:** $H_{ij} = H_{ji}$ (mixed partials are equal)\n",
    "3. **Diagonal:** Curvature in single variable directions\n",
    "4. **Off-diagonal:** Variable interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute Hessian by hand vs PyTorch\n",
    "\n",
    "# f(x, y) = x² + 3xy + 2y²\n",
    "# ∂f/∂x = 2x + 3y,  ∂f/∂y = 3x + 4y\n",
    "# ∂²f/∂x² = 2,  ∂²f/∂y² = 4,  ∂²f/∂x∂y = 3\n",
    "\n",
    "def f(xy):\n",
    "    x, y = xy[0], xy[1]\n",
    "    return x**2 + 3*x*y + 2*y**2\n",
    "\n",
    "# Compute Hessian at any point (it's constant for this quadratic)\n",
    "point = torch.tensor([1.0, 1.0])\n",
    "H = hessian(f, point)\n",
    "\n",
    "print(\"Function: f(x,y) = x² + 3xy + 2y²\")\n",
    "print(\"\\nHessian (computed by PyTorch):\")\n",
    "print(H)\n",
    "\n",
    "print(\"\\nExpected (by hand):\")\n",
    "print(\"[[2, 3],\")\n",
    "print(\" [3, 4]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Quadratic Form and Definiteness\n",
    "\n",
    "The **quadratic form** $\\vec{v}^\\top H \\vec{v}$ tells us about curvature in direction $\\vec{v}$.\n",
    "\n",
    "### Definiteness Classification\n",
    "\n",
    "| Type | Condition | Geometric Meaning | At Critical Point |\n",
    "|------|-----------|-------------------|-------------------|\n",
    "| Positive definite | $\\vec{v}^\\top H \\vec{v} > 0$ for all $\\vec{v} \\neq 0$ | Bowl (curves up) | **Minimum** |\n",
    "| Negative definite | $\\vec{v}^\\top H \\vec{v} < 0$ for all $\\vec{v} \\neq 0$ | Dome (curves down) | **Maximum** |\n",
    "| Indefinite | Mixed signs | Saddle | **Saddle point** |\n",
    "\n",
    "### 2×2 Shortcut\n",
    "\n",
    "For $H = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}$:\n",
    "\n",
    "- **Positive definite:** $a > 0$ AND $ac - b^2 > 0$\n",
    "- **Negative definite:** $a < 0$ AND $ac - b^2 > 0$\n",
    "- **Indefinite:** $ac - b^2 < 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_definiteness(H):\n",
    "    \"\"\"Check definiteness of a 2x2 symmetric matrix.\"\"\"\n",
    "    a, b, c = H[0,0].item(), H[0,1].item(), H[1,1].item()\n",
    "    det = a * c - b**2\n",
    "    \n",
    "    print(f\"Matrix: [[{a}, {b}], [{b}, {c}]]\")\n",
    "    print(f\"a = {a}, det = ac - b² = {det:.4f}\")\n",
    "    \n",
    "    # Also check eigenvalues (general method)\n",
    "    eigenvalues = torch.linalg.eigvalsh(H)\n",
    "    print(f\"Eigenvalues: {eigenvalues.tolist()}\")\n",
    "    \n",
    "    if det > 0 and a > 0:\n",
    "        return \"Positive definite → MINIMUM\"\n",
    "    elif det > 0 and a < 0:\n",
    "        return \"Negative definite → MAXIMUM\"\n",
    "    elif det < 0:\n",
    "        return \"Indefinite → SADDLE POINT\"\n",
    "    else:\n",
    "        return \"Semidefinite (boundary case)\"\n",
    "\n",
    "# Test with our Hessian\n",
    "H1 = torch.tensor([[2.0, 3.0], [3.0, 4.0]])\n",
    "print(\"Example 1: f(x,y) = x² + 3xy + 2y²\")\n",
    "print(f\"Result: {check_definiteness(H1)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# A positive definite example\n",
    "H2 = torch.tensor([[2.0, 0.0], [0.0, 4.0]])\n",
    "print(\"Example 2: f(x,y) = x² + 2y² (simple bowl)\")\n",
    "print(f\"Result: {check_definiteness(H2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Visualizing Different Surface Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create three functions: minimum, maximum, saddle\n",
    "x = np.linspace(-2, 2, 50)\n",
    "y = np.linspace(-2, 2, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Minimum (positive definite Hessian)\n",
    "Z_min = X**2 + Y**2\n",
    "\n",
    "# Maximum (negative definite Hessian)\n",
    "Z_max = -(X**2 + Y**2)\n",
    "\n",
    "# Saddle (indefinite Hessian)\n",
    "Z_saddle = X**2 - Y**2\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "titles = ['Minimum\\n$f = x^2 + y^2$\\nH positive definite',\n",
    "          'Maximum\\n$f = -x^2 - y^2$\\nH negative definite',\n",
    "          'Saddle Point\\n$f = x^2 - y^2$\\nH indefinite']\n",
    "surfaces = [Z_min, Z_max, Z_saddle]\n",
    "\n",
    "for i, (Z, title) in enumerate(zip(surfaces, titles)):\n",
    "    ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "    ax.scatter([0], [0], [0], color='red', s=100, label='Critical point')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('f(x,y)')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Taylor Expansion (Multivariate)\n",
    "\n",
    "The second-order Taylor expansion around $\\vec{x}_k$:\n",
    "\n",
    "$$f(\\vec{x}) \\approx f(\\vec{x}_k) + \\nabla f(\\vec{x}_k)^\\top (\\vec{x} - \\vec{x}_k) + \\frac{1}{2}(\\vec{x} - \\vec{x}_k)^\\top H_f(\\vec{x}_k) (\\vec{x} - \\vec{x}_k)$$\n",
    "\n",
    "### Breaking it down:\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| $f(\\vec{x}_k)$ | Value at current point |\n",
    "| $\\nabla f^\\top \\Delta\\vec{x}$ | First-order (linear) change |\n",
    "| $\\frac{1}{2}\\Delta\\vec{x}^\\top H \\Delta\\vec{x}$ | Second-order (curvature) correction |\n",
    "\n",
    "**Key insight:** For quadratic functions, this approximation is **exact**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Taylor approximation\n",
    "def f_nonquadratic(xy):\n",
    "    x, y = xy[0], xy[1]\n",
    "    return torch.sin(x) + torch.cos(y) + 0.1*x**2\n",
    "\n",
    "def taylor_approximation(f, x0, dx):\n",
    "    \"\"\"Compute second-order Taylor approximation.\"\"\"\n",
    "    x0 = x0.clone().requires_grad_(True)\n",
    "    \n",
    "    # f(x0)\n",
    "    f0 = f(x0)\n",
    "    \n",
    "    # Gradient\n",
    "    f0.backward()\n",
    "    grad = x0.grad.clone()\n",
    "    \n",
    "    # Hessian\n",
    "    H = hessian(f, x0.detach())\n",
    "    \n",
    "    # Taylor approximation\n",
    "    linear_term = torch.dot(grad, dx)\n",
    "    quadratic_term = 0.5 * dx @ H @ dx\n",
    "    \n",
    "    return f0.item() + linear_term.item() + quadratic_term.item()\n",
    "\n",
    "# Compare true vs approximation\n",
    "x0 = torch.tensor([0.5, 0.5])\n",
    "dx = torch.tensor([0.1, 0.1])\n",
    "\n",
    "true_value = f_nonquadratic(x0 + dx).item()\n",
    "approx_value = taylor_approximation(f_nonquadratic, x0, dx)\n",
    "\n",
    "print(f\"Point: x0 = {x0.tolist()}, step = {dx.tolist()}\")\n",
    "print(f\"True f(x0 + dx): {true_value:.6f}\")\n",
    "print(f\"Taylor approx:   {approx_value:.6f}\")\n",
    "print(f\"Error:           {abs(true_value - approx_value):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Newton's Method\n",
    "\n",
    "### Derivation\n",
    "\n",
    "1. Approximate $f$ with quadratic Taylor expansion\n",
    "2. Set gradient of approximation to zero\n",
    "3. Solve for the step $\\Delta\\vec{x}$\n",
    "\n",
    "$$\\nabla f + H\\Delta\\vec{x} = 0$$\n",
    "$$\\Delta\\vec{x} = -H^{-1}\\nabla f$$\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "$$\\vec{x}_{k+1} = \\vec{x}_k - H_f^{-1}(\\vec{x}_k) \\nabla f(\\vec{x}_k)$$\n",
    "\n",
    "### Comparison with Gradient Descent\n",
    "\n",
    "| | Gradient Descent | Newton's Method |\n",
    "|---|-----------------|----------------|\n",
    "| Update | $\\vec{x} - \\alpha \\nabla f$ | $\\vec{x} - H^{-1}\\nabla f$ |\n",
    "| Step size | Manual ($\\alpha$) | Automatic ($H^{-1}$) |\n",
    "| Convergence | Linear | Quadratic (near optimum) |\n",
    "| Cost per step | $O(n)$ | $O(n^3)$ |\n",
    "| Best for | Large-scale | Small-scale convex |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, x0, lr=0.1, max_iter=100, tol=1e-6):\n",
    "    \"\"\"Gradient descent optimization.\"\"\"\n",
    "    x = x0.clone()\n",
    "    path = [x.clone().numpy()]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        x = x.requires_grad_(True)\n",
    "        loss = f(x)\n",
    "        loss.backward()\n",
    "        grad = x.grad.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = x - lr * grad\n",
    "        \n",
    "        path.append(x.clone().numpy())\n",
    "        \n",
    "        if torch.norm(grad) < tol:\n",
    "            break\n",
    "    \n",
    "    return x, np.array(path)\n",
    "\n",
    "\n",
    "def newtons_method(f, x0, max_iter=100, tol=1e-6):\n",
    "    \"\"\"Newton's method optimization.\"\"\"\n",
    "    x = x0.clone()\n",
    "    path = [x.clone().numpy()]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        x = x.requires_grad_(True)\n",
    "        loss = f(x)\n",
    "        loss.backward()\n",
    "        grad = x.grad.clone()\n",
    "        \n",
    "        # Compute Hessian\n",
    "        H = hessian(f, x.detach())\n",
    "        \n",
    "        # Newton step: solve H @ step = -grad\n",
    "        step = torch.linalg.solve(H, -grad)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = x + step\n",
    "        \n",
    "        path.append(x.clone().numpy())\n",
    "        \n",
    "        if torch.norm(grad) < tol:\n",
    "            break\n",
    "    \n",
    "    return x, np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on an elongated bowl (harder for GD)\n",
    "def elongated_bowl(xy):\n",
    "    x, y = xy[0], xy[1]\n",
    "    return x**2 + 10*y**2  # Very different curvature in x vs y\n",
    "\n",
    "x0 = torch.tensor([5.0, 5.0])\n",
    "\n",
    "# Run both methods\n",
    "x_gd, path_gd = gradient_descent(elongated_bowl, x0, lr=0.05, max_iter=50)\n",
    "x_newton, path_newton = newtons_method(elongated_bowl, x0, max_iter=10)\n",
    "\n",
    "print(f\"Gradient Descent: {len(path_gd)-1} iterations\")\n",
    "print(f\"  Final point: ({path_gd[-1][0]:.6f}, {path_gd[-1][1]:.6f})\")\n",
    "print(f\"\\nNewton's Method: {len(path_newton)-1} iterations\")\n",
    "print(f\"  Final point: ({path_newton[-1][0]:.6f}, {path_newton[-1][1]:.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the paths\n",
    "x_range = np.linspace(-6, 6, 100)\n",
    "y_range = np.linspace(-6, 6, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 + 10*Y**2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, path, title in [(axes[0], path_gd, 'Gradient Descent'),\n",
    "                         (axes[1], path_newton, \"Newton's Method\")]:\n",
    "    ax.contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "    ax.plot(path[:, 0], path[:, 1], 'ro-', markersize=4, linewidth=1.5, label='Path')\n",
    "    ax.plot(path[0, 0], path[0, 1], 'go', markersize=10, label='Start')\n",
    "    ax.plot(path[-1, 0], path[-1, 1], 'b*', markersize=15, label='End')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f\"{title}\\n({len(path)-1} iterations)\")\n",
    "    ax.legend()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Why Newton Converges in 1 Step for Quadratics\n",
    "\n",
    "For a **quadratic function**, the Taylor expansion is **exact** (no higher-order terms).\n",
    "\n",
    "Therefore:\n",
    "- Newton finds the exact minimum of the quadratic approximation\n",
    "- Since approximation = true function, Newton finds the true minimum\n",
    "- **One step is enough!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: Newton converges in 1 step for quadratic\n",
    "def quadratic(xy):\n",
    "    x, y = xy[0], xy[1]\n",
    "    return x**2 + 10*y**2 + 3*x + 5*y + 7  # Quadratic with linear terms\n",
    "\n",
    "x0 = torch.tensor([10.0, 10.0])\n",
    "x_result, path = newtons_method(quadratic, x0, max_iter=5)\n",
    "\n",
    "print(\"Newton's method on quadratic function:\")\n",
    "for i, p in enumerate(path):\n",
    "    f_val = quadratic(torch.tensor(p)).item()\n",
    "    print(f\"  Step {i}: x = ({p[0]:.6f}, {p[1]:.6f}), f(x) = {f_val:.6f}\")\n",
    "\n",
    "print(\"\\n✓ Converged in 1 step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Genomics Application: When to Use Newton vs GD\n",
    "\n",
    "| Tool | Method | Why |\n",
    "|------|--------|-----|\n",
    "| **DESeq2** | Newton-like (IRLS) | Few parameters per gene, convex GLM |\n",
    "| **edgeR** | Newton-like | Same reasons |\n",
    "| **scVI** | Adam (GD variant) | Millions of parameters, non-convex |\n",
    "| **Neural networks** | SGD/Adam | Can't store/invert Hessian |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating why Newton fails at scale\n",
    "\n",
    "def memory_comparison(n_params):\n",
    "    \"\"\"Compare memory requirements.\"\"\"\n",
    "    gradient_memory = n_params * 4 / (1024**3)  # 4 bytes per float, convert to GB\n",
    "    hessian_memory = n_params**2 * 4 / (1024**3)\n",
    "    return gradient_memory, hessian_memory\n",
    "\n",
    "print(\"Memory Requirements Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Parameters':<15} {'Gradient':<15} {'Hessian':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for n in [100, 1000, 10000, 100000, 1000000]:\n",
    "    grad_mem, hess_mem = memory_comparison(n)\n",
    "    grad_str = f\"{grad_mem*1000:.2f} MB\" if grad_mem < 1 else f\"{grad_mem:.2f} GB\"\n",
    "    hess_str = f\"{hess_mem*1000:.2f} MB\" if hess_mem < 1 else f\"{hess_mem:.2f} GB\"\n",
    "    if hess_mem > 1000:\n",
    "        hess_str = f\"{hess_mem/1000:.0f} TB ✗\"\n",
    "    print(f\"{n:<15,} {grad_str:<15} {hess_str:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Hessian Computation\n",
    "\n",
    "Compute the Hessian of $f(x, y) = e^x \\cos(y)$ at the point $(0, 0)$.\n",
    "Is it positive definite, negative definite, or indefinite?\n",
    "\n",
    "### Exercise 2: Critical Point Classification\n",
    "\n",
    "For $f(x, y) = x^3 - 3xy + y^3$:\n",
    "1. Find all critical points (where $\\nabla f = 0$)\n",
    "2. Compute the Hessian at each critical point\n",
    "3. Classify each as min/max/saddle\n",
    "\n",
    "### Exercise 3: Newton vs GD Comparison\n",
    "\n",
    "Create a function where gradient descent struggles but Newton succeeds quickly. Hint: Use very different curvatures in different directions.\n",
    "\n",
    "### Exercise 4: DESeq2 Intuition\n",
    "\n",
    "A negative binomial GLM has approximately 3 parameters per gene (intercept, treatment effect, dispersion). For 20,000 genes, how large would the full Hessian be if we optimized all genes jointly? Why does DESeq2 optimize genes independently instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| **Hessian** | Matrix of second derivatives, shape $(n \\times n)$ |\n",
    "| **Positive definite** | All eigenvalues > 0 → minimum |\n",
    "| **Indefinite** | Mixed eigenvalues → saddle point |\n",
    "| **Newton's method** | $\\vec{x}_{k+1} = \\vec{x}_k - H^{-1}\\nabla f$ |\n",
    "| **Newton vs GD** | Newton is faster but costs $O(n^3)$ |\n",
    "| **Quadratic functions** | Newton converges in 1 step |\n",
    "\n",
    "## Next Notebook\n",
    "\n",
    "**03_jacobian_backprop.ipynb:** Jacobian matrices for vector-valued functions and backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
