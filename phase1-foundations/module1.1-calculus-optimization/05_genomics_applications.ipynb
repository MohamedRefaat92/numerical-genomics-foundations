{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Genomics Applications\n",
    "\n",
    "**Module 1.1: Calculus & Optimization - Integration**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook ties together all concepts from Module 1.1 with real genomics applications:\n",
    "1. Understand DESeq2's IRLS algorithm (Newton-like optimization)\n",
    "2. See why condition numbers matter for stable analysis\n",
    "3. Compare convex (DESeq2) vs non-convex (scVI) optimization\n",
    "4. Implement gradient descent for gene expression prediction\n",
    "\n",
    "## Resources\n",
    "- DESeq2 paper: Love et al., 2014\n",
    "- scVI paper: Lopez et al., 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd.functional import hessian\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Optimization Landscape in Genomics\n",
    "\n",
    "| Tool | Method | Convex? | Reproducible? |\n",
    "|------|--------|---------|---------------|\n",
    "| **DESeq2** | IRLS (Newton-like) | ‚úì Yes | ‚úì Same result every time |\n",
    "| **edgeR** | IRLS | ‚úì Yes | ‚úì Same result every time |\n",
    "| **limma** | Least squares | ‚úì Yes | ‚úì Same result every time |\n",
    "| **scVI** | Adam (SGD) | ‚úó No | ‚ö†Ô∏è Seed-dependent |\n",
    "| **scANVI** | Adam | ‚úó No | ‚ö†Ô∏è Seed-dependent |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convex vs non-convex\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Convex (DESeq2-like)\n",
    "y_convex = x**2 + 1\n",
    "axes[0].plot(x, y_convex, 'b-', linewidth=2)\n",
    "axes[0].scatter([0], [1], color='green', s=200, zorder=5, label='Global minimum')\n",
    "axes[0].set_title('Convex Loss (DESeq2, limma)\\nOne minimum, always found', fontsize=12)\n",
    "axes[0].set_xlabel('Parameter')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Non-convex (Neural network-like)\n",
    "y_nonconvex = np.sin(3*x) + 0.5*x**2\n",
    "axes[1].plot(x, y_nonconvex, 'r-', linewidth=2)\n",
    "local_mins = [-1.9, -0.1, 1.7]\n",
    "for lm in local_mins:\n",
    "    y_lm = np.sin(3*lm) + 0.5*lm**2\n",
    "    axes[1].scatter([lm], [y_lm], color='orange', s=150, zorder=5)\n",
    "axes[1].scatter([-0.1], [np.sin(3*-0.1) + 0.5*(-0.1)**2], color='green', s=200, zorder=5, label='Global minimum')\n",
    "axes[1].scatter([1.7], [np.sin(3*1.7) + 0.5*(1.7)**2], color='orange', s=150, zorder=5, label='Local minima')\n",
    "axes[1].set_title('Non-convex Loss (scVI, Neural Nets)\\nMultiple minima, result depends on initialization', fontsize=12)\n",
    "axes[1].set_xlabel('Parameter')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DESeq2's IRLS Algorithm\n",
    "\n",
    "DESeq2 uses **Iteratively Reweighted Least Squares** - a Newton-like method for GLMs.\n",
    "\n",
    "### The Model\n",
    "\n",
    "$$K_{ij} \\sim \\text{NegativeBinomial}(\\mu_{ij}, \\alpha_i)$$\n",
    "\n",
    "$$\\log(\\mu_{ij}) = X_j \\cdot \\beta_i$$\n",
    "\n",
    "where:\n",
    "- $K_{ij}$ = counts for gene $i$, sample $j$\n",
    "- $\\mu_{ij}$ = expected count\n",
    "- $\\alpha_i$ = dispersion for gene $i$\n",
    "- $X$ = design matrix\n",
    "- $\\beta_i$ = coefficients for gene $i$\n",
    "\n",
    "### IRLS Update\n",
    "\n",
    "$$\\beta^{(t+1)} = (X^T W X)^{-1} X^T W z$$\n",
    "\n",
    "where $W$ = diagonal weight matrix, $z$ = working response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified IRLS for Poisson regression (simpler than NB)\n",
    "def irls_poisson(X, y, max_iter=25, tol=1e-8):\n",
    "    \"\"\"\n",
    "    IRLS for Poisson regression: y ~ Poisson(exp(X @ beta))\n",
    "    \n",
    "    This is the core of what DESeq2 does (with NB instead of Poisson).\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    beta = np.zeros(p)  # Initialize at zero\n",
    "    \n",
    "    history = {'beta': [beta.copy()], 'deviance': []}\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Current predictions\n",
    "        eta = X @ beta\n",
    "        mu = np.exp(eta)\n",
    "        \n",
    "        # Weights (for Poisson: W = diag(mu))\n",
    "        W = np.diag(mu)\n",
    "        \n",
    "        # Working response\n",
    "        z = eta + (y - mu) / mu\n",
    "        \n",
    "        # IRLS update: solve weighted least squares\n",
    "        # (X'WX) beta = X'Wz\n",
    "        XtWX = X.T @ W @ X\n",
    "        XtWz = X.T @ W @ z\n",
    "        \n",
    "        beta_new = np.linalg.solve(XtWX, XtWz)\n",
    "        \n",
    "        # Check convergence\n",
    "        change = np.max(np.abs(beta_new - beta))\n",
    "        beta = beta_new\n",
    "        \n",
    "        # Deviance (goodness of fit)\n",
    "        deviance = 2 * np.sum(y * np.log(y / mu + 1e-10) - (y - mu))\n",
    "        \n",
    "        history['beta'].append(beta.copy())\n",
    "        history['deviance'].append(deviance)\n",
    "        \n",
    "        if change < tol:\n",
    "            print(f\"Converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return beta, history\n",
    "\n",
    "# Simulate gene expression data\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "# Design matrix: intercept + treatment\n",
    "X = np.column_stack([\n",
    "    np.ones(n_samples),           # Intercept\n",
    "    np.array([0]*25 + [1]*25)     # Treatment (0 = control, 1 = treated)\n",
    "])\n",
    "\n",
    "# True coefficients: baseline = 5 (log scale), treatment effect = 1\n",
    "beta_true = np.array([5.0, 1.0])\n",
    "\n",
    "# Generate counts\n",
    "mu_true = np.exp(X @ beta_true)\n",
    "y = np.random.poisson(mu_true)\n",
    "\n",
    "print(\"Simulated gene expression analysis\")\n",
    "print(f\"True coefficients: {beta_true}\")\n",
    "print(f\"Control mean count: {np.exp(beta_true[0]):.1f}\")\n",
    "print(f\"Treatment fold change: {np.exp(beta_true[1]):.2f}x\")\n",
    "\n",
    "# Run IRLS\n",
    "beta_est, history = irls_poisson(X, y)\n",
    "print(f\"\\nEstimated coefficients: {beta_est}\")\n",
    "print(f\"Estimated fold change: {np.exp(beta_est[1]):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize IRLS convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "betas = np.array(history['beta'])\n",
    "axes[0].plot(betas[:, 0], 'b-o', label='Œ≤‚ÇÄ (intercept)')\n",
    "axes[0].plot(betas[:, 1], 'r-o', label='Œ≤‚ÇÅ (treatment)')\n",
    "axes[0].axhline(beta_true[0], color='b', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(beta_true[1], color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Coefficient value')\n",
    "axes[0].set_title('IRLS Convergence')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['deviance'], 'g-o')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Deviance')\n",
    "axes[1].set_title('Deviance (lower = better fit)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Condition Numbers: When Analysis Fails\n",
    "\n",
    "The **condition number** $\\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}$ tells us how numerically stable a matrix is.\n",
    "\n",
    "- $\\kappa \\approx 1$: Well-conditioned, stable\n",
    "- $\\kappa > 10^{10}$: Ill-conditioned, numerical issues\n",
    "- $\\kappa = \\infty$: Singular, no unique solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_design_matrix(X, name=\"\"):\n",
    "    \"\"\"Analyze numerical stability of design matrix.\"\"\"\n",
    "    # SVD\n",
    "    U, S, Vh = np.linalg.svd(X)\n",
    "    \n",
    "    # Condition number\n",
    "    kappa = S[0] / S[-1] if S[-1] > 1e-15 else np.inf\n",
    "    \n",
    "    # Rank\n",
    "    rank = np.sum(S > 1e-10)\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Shape: {X.shape}\")\n",
    "    print(f\"  Rank: {rank} / {min(X.shape)}\")\n",
    "    print(f\"  Condition number: {kappa:.2e}\")\n",
    "    print(f\"  Singular values: {S[:5].round(4)}...\")\n",
    "    \n",
    "    if kappa > 1e10:\n",
    "        print(\"  ‚ö†Ô∏è WARNING: Ill-conditioned! Results may be unstable.\")\n",
    "    elif kappa > 1e6:\n",
    "        print(\"  ‚ö†Ô∏è CAUTION: Moderately ill-conditioned.\")\n",
    "    else:\n",
    "        print(\"  ‚úì Well-conditioned.\")\n",
    "    \n",
    "    return kappa\n",
    "\n",
    "# Good design matrix\n",
    "X_good = np.column_stack([\n",
    "    np.ones(100),\n",
    "    np.array([0]*50 + [1]*50),\n",
    "    np.random.randn(100)\n",
    "])\n",
    "analyze_design_matrix(X_good, \"Good design (balanced, independent)\")\n",
    "\n",
    "# Collinear design (bad!)\n",
    "X_collinear = np.column_stack([\n",
    "    np.ones(100),\n",
    "    np.arange(100),\n",
    "    np.arange(100) + np.random.randn(100) * 0.001  # Almost identical to previous\n",
    "])\n",
    "analyze_design_matrix(X_collinear, \"Collinear design (batch ‚âà time)\")\n",
    "\n",
    "# Imbalanced design\n",
    "X_imbalanced = np.column_stack([\n",
    "    np.ones(100),\n",
    "    np.array([0]*99 + [1]*1)  # Only 1 treated sample!\n",
    "])\n",
    "analyze_design_matrix(X_imbalanced, \"Imbalanced design (1 vs 99)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Neural Network for Gene Expression (scVI-like)\n",
    "\n",
    "Unlike DESeq2's convex optimization, neural networks have non-convex losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple autoencoder for gene expression\n",
    "class GeneExpressionAutoencoder(nn.Module):\n",
    "    def __init__(self, n_genes, n_latent=10):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_genes, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_latent)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_genes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# Simulate gene expression data\n",
    "n_genes = 100\n",
    "n_cells = 500\n",
    "\n",
    "# Create data with structure (two cell types)\n",
    "torch.manual_seed(42)\n",
    "cell_type = torch.randint(0, 2, (n_cells,))\n",
    "X = torch.randn(n_cells, n_genes)\n",
    "X[cell_type == 1, :50] += 2  # Cell type 1 has higher expression of first 50 genes\n",
    "\n",
    "print(f\"Data: {n_cells} cells √ó {n_genes} genes\")\n",
    "print(f\"Cell types: {(cell_type == 0).sum()} type A, {(cell_type == 1).sum()} type B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different random seeds ‚Üí different results!\n",
    "def train_autoencoder(X, seed, epochs=100):\n",
    "    torch.manual_seed(seed)\n",
    "    model = GeneExpressionAutoencoder(n_genes=X.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        X_recon = model(X)\n",
    "        loss = nn.MSELoss()(X_recon, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "# Train with 3 different seeds\n",
    "results = {}\n",
    "for seed in [1, 42, 123]:\n",
    "    model, losses = train_autoencoder(X, seed)\n",
    "    results[seed] = {'model': model, 'losses': losses, 'final_loss': losses[-1]}\n",
    "    print(f\"Seed {seed}: Final loss = {losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Different seeds ‚Üí different final losses!\")\n",
    "print(\"   This is why scVI results vary with random seed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different training trajectories\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for seed, data in results.items():\n",
    "    plt.plot(data['losses'], label=f'Seed {seed} (final: {data[\"final_loss\"]:.4f})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Non-convex optimization: Different seeds ‚Üí Different local minima')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Checking the Hessian at Convergence\n",
    "\n",
    "At a proper minimum, the Hessian should be **positive definite**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a simple model, check Hessian at convergence\n",
    "def simple_loss(params, X, y):\n",
    "    \"\"\"Simple quadratic loss.\"\"\"\n",
    "    return torch.mean((X @ params - y)**2)\n",
    "\n",
    "# Generate data\n",
    "torch.manual_seed(42)\n",
    "n, p = 100, 3\n",
    "X_data = torch.randn(n, p)\n",
    "true_params = torch.tensor([1.0, -0.5, 2.0])\n",
    "y_data = X_data @ true_params + 0.1 * torch.randn(n)\n",
    "\n",
    "# Find optimal parameters\n",
    "params = torch.randn(p, requires_grad=True)\n",
    "optimizer = torch.optim.Adam([params], lr=0.1)\n",
    "\n",
    "for _ in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    loss = simple_loss(params, X_data, y_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Optimized params: {params.data.numpy().round(3)}\")\n",
    "print(f\"True params:      {true_params.numpy()}\")\n",
    "\n",
    "# Compute Hessian at solution\n",
    "def loss_fn(p):\n",
    "    return simple_loss(p, X_data, y_data)\n",
    "\n",
    "H = hessian(loss_fn, params.detach())\n",
    "eigenvalues = torch.linalg.eigvalsh(H)\n",
    "\n",
    "print(f\"\\nHessian eigenvalues: {eigenvalues.numpy().round(4)}\")\n",
    "if torch.all(eigenvalues > 0):\n",
    "    print(\"‚úì All positive ‚Üí Proper minimum!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not all positive ‚Üí Saddle point or maximum!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Module 1.1 Complete!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "| Concept | Genomics Application |\n",
    "|---------|----------------------|\n",
    "| **Gradient** | Direction to update model parameters |\n",
    "| **Hessian** | Curvature, used in IRLS (DESeq2) |\n",
    "| **Positive definite** | Confirms you found a minimum |\n",
    "| **Jacobian** | Backprop through neural network layers |\n",
    "| **Autodiff** | How PyTorch computes gradients |\n",
    "| **Condition number** | Numerical stability of design matrix |\n",
    "| **Convex vs non-convex** | Why DESeq2 is reproducible but scVI isn't |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **DESeq2/edgeR:** Newton-like methods (IRLS), convex optimization, reproducible\n",
    "2. **scVI/Neural nets:** Gradient descent, non-convex, seed-dependent\n",
    "3. **Condition numbers:** Check design matrix before running analysis\n",
    "4. **Hessian eigenvalues:** Verify you're at a minimum, not saddle point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: IRLS for Logistic Regression\n",
    "Modify the IRLS code for binary classification (cell type A vs B).\n",
    "\n",
    "### Exercise 2: Condition Number Experiments\n",
    "Create design matrices with varying condition numbers and see how this affects coefficient estimates.\n",
    "\n",
    "### Exercise 3: Reproducibility Test\n",
    "Train the autoencoder 10 times with different seeds. Plot the distribution of final losses.\n",
    "\n",
    "### Exercise 4: Real Data\n",
    "Load a real scRNA-seq dataset and analyze the design matrix condition number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Module 1.1 Complete!\n",
    "\n",
    "**Next:** Module 1.2 - Linear Systems & Least Squares\n",
    "\n",
    "You'll learn:\n",
    "- LU and QR decomposition\n",
    "- Why QR is more stable than normal equations\n",
    "- Ridge and LASSO regularization\n",
    "- Applications to gene expression modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
